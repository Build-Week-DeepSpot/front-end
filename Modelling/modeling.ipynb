{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "# from ipynb.fs.full.Preprocessing import clean_data\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# df = pd.read_csv('./Data/songs_cleaned.csv', index_col='Unnamed: 0')\n",
    "# print(df.shape)\n",
    "# df.describe()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df = pd.read_csv('../../songs_cleaned.csv', index_col='Unnamed: 0')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Simple autoencoder\n",
    "\n",
    "encoded_dim = 16\n",
    "\n",
    "input_song = Input(shape = (df.shape[1], ))\n",
    "\n",
    "h1 = Dense(32, activation = 'relu')(input_song)\n",
    "\n",
    "encoded = Dense(encoded_dim, activation = 'relu')(h1)\n",
    "\n",
    "dh1 = Dense(32, activation='sigmoid')(encoded)\n",
    "\n",
    "decoded = Dense(df.shape[1], activation='sigmoid')(dh1)\n",
    "\n",
    "autoencoder = Model(input_song, decoded)\n",
    "\n",
    "encoder = Model(input_song, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='nadam', loss='mean_squared_error')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3)\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "logdir = os.path.join(\"logs\", f\"SimpleAutoencoder-{now}\")\n",
    "tensorboard = TensorBoard(log_dir=logdir)\n",
    "\n",
    "autoencoder.fit(df, # input image to encoder\n",
    "                df, # provide input image to decoder so the model learns how to reconstruct the input image \n",
    "                epochs=100,\n",
    "                validation_split=.2,\n",
    "                callbacks=[stop, tensorboard])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 469075 samples, validate on 117269 samples\n",
      "Epoch 1/100\n",
      "469075/469075 [==============================] - 23s 49us/sample - loss: 0.3371 - val_loss: 0.3316\n",
      "Epoch 2/100\n",
      "469075/469075 [==============================] - 23s 48us/sample - loss: 0.3011 - val_loss: 0.3211\n",
      "Epoch 3/100\n",
      "469075/469075 [==============================] - 23s 50us/sample - loss: 0.2959 - val_loss: 0.3181\n",
      "Epoch 4/100\n",
      "469075/469075 [==============================] - 24s 51us/sample - loss: 0.2946 - val_loss: 0.3174\n",
      "Epoch 5/100\n",
      "469075/469075 [==============================] - 23s 48us/sample - loss: 0.2940 - val_loss: 0.3169\n",
      "Epoch 6/100\n",
      "469075/469075 [==============================] - 23s 49us/sample - loss: 0.2938 - val_loss: 0.3168\n",
      "Epoch 7/100\n",
      "469075/469075 [==============================] - 24s 51us/sample - loss: 0.2937 - val_loss: 0.3167\n",
      "Epoch 8/100\n",
      "469075/469075 [==============================] - 24s 51us/sample - loss: 0.2936 - val_loss: 0.3165\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf114a9fd0>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shallow NN Summary:\n",
    "0 hidden layers: Val Loss: 0.3160\n",
    "1 hidden layer: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "decoded_songs = autoencoder.predict(df)\n",
    "encoded_songs = encoder.predict(df)\n",
    "\n",
    "embeddings_df = pd.DataFrame(encoded_songs, index=df.index)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(embeddings_df.shape)\n",
    "embeddings_df"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(586344, 16)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              0         1    2         3         4         5         6   \\\n",
       "0       3.470329  1.184762 -0.0  3.144354  1.041142  2.506661  2.963388   \n",
       "1       2.950846  0.692970 -0.0  1.260800  2.372062  1.424065  1.766102   \n",
       "2       4.472703  6.916587 -0.0  3.051311  4.255427  4.078246  6.610500   \n",
       "3       0.533942  1.481426 -0.0  4.247745  1.795213  3.724570  6.466702   \n",
       "4       2.028276  4.151319 -0.0  2.583655  2.820859  2.327025  2.896990   \n",
       "...          ...       ...  ...       ...       ...       ...       ...   \n",
       "586667  4.507507  4.525337 -0.0  1.797992  4.408144  3.942313  3.207572   \n",
       "586668  4.967984  3.997606 -0.0  2.682946  5.384268  4.684574  2.361519   \n",
       "586669  2.513383  4.620409 -0.0  3.026764  4.275283  6.560318  5.074978   \n",
       "586670  4.446574  4.300193 -0.0  4.233162  4.264293  5.235272  1.241963   \n",
       "586671  3.847120  3.737806 -0.0  2.963249  5.151583  2.118279  3.689731   \n",
       "\n",
       "              7         8         9         10        11        12        13  \\\n",
       "0       1.308488  1.876889  1.609339  5.813118  4.160755  2.801373  6.369508   \n",
       "1       3.973808  1.751205  1.503640  4.932892  4.102685  1.840484  4.968472   \n",
       "2       5.424572  5.226394  2.885791  4.807835  8.209723  6.890685  2.668237   \n",
       "3       2.883682  2.688429  2.568929  3.822564  2.750004  4.210648  3.195352   \n",
       "4       3.921444  3.261746  1.292208  6.060862  3.102732  4.927852  3.230224   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "586667  3.014376  2.259826  3.485083  7.321132  4.829626  4.268849  4.802366   \n",
       "586668  3.863641  2.298209  4.028028  7.587534  5.696170  5.347732  3.508698   \n",
       "586669  4.096501  2.595194  1.959881  3.780759  4.040783  5.324980  2.569675   \n",
       "586670  4.270548  4.267136  2.311171  6.389156  5.381336  5.270690  4.331300   \n",
       "586671  4.534883  3.179928  3.346431  3.523212  3.196295  6.148501  0.977880   \n",
       "\n",
       "              14        15  \n",
       "0       4.479137  2.048349  \n",
       "1       4.601698  2.108422  \n",
       "2       4.231436  3.645994  \n",
       "3       3.263384  3.226213  \n",
       "4       1.068353  2.544864  \n",
       "...          ...       ...  \n",
       "586667  3.543452  2.489765  \n",
       "586668  5.025582  3.510531  \n",
       "586669  2.156150  2.363571  \n",
       "586670  3.387321  5.655649  \n",
       "586671  3.550117  3.168971  \n",
       "\n",
       "[586344 rows x 16 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.470329</td>\n",
       "      <td>1.184762</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3.144354</td>\n",
       "      <td>1.041142</td>\n",
       "      <td>2.506661</td>\n",
       "      <td>2.963388</td>\n",
       "      <td>1.308488</td>\n",
       "      <td>1.876889</td>\n",
       "      <td>1.609339</td>\n",
       "      <td>5.813118</td>\n",
       "      <td>4.160755</td>\n",
       "      <td>2.801373</td>\n",
       "      <td>6.369508</td>\n",
       "      <td>4.479137</td>\n",
       "      <td>2.048349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.950846</td>\n",
       "      <td>0.692970</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.260800</td>\n",
       "      <td>2.372062</td>\n",
       "      <td>1.424065</td>\n",
       "      <td>1.766102</td>\n",
       "      <td>3.973808</td>\n",
       "      <td>1.751205</td>\n",
       "      <td>1.503640</td>\n",
       "      <td>4.932892</td>\n",
       "      <td>4.102685</td>\n",
       "      <td>1.840484</td>\n",
       "      <td>4.968472</td>\n",
       "      <td>4.601698</td>\n",
       "      <td>2.108422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.472703</td>\n",
       "      <td>6.916587</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3.051311</td>\n",
       "      <td>4.255427</td>\n",
       "      <td>4.078246</td>\n",
       "      <td>6.610500</td>\n",
       "      <td>5.424572</td>\n",
       "      <td>5.226394</td>\n",
       "      <td>2.885791</td>\n",
       "      <td>4.807835</td>\n",
       "      <td>8.209723</td>\n",
       "      <td>6.890685</td>\n",
       "      <td>2.668237</td>\n",
       "      <td>4.231436</td>\n",
       "      <td>3.645994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.533942</td>\n",
       "      <td>1.481426</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>4.247745</td>\n",
       "      <td>1.795213</td>\n",
       "      <td>3.724570</td>\n",
       "      <td>6.466702</td>\n",
       "      <td>2.883682</td>\n",
       "      <td>2.688429</td>\n",
       "      <td>2.568929</td>\n",
       "      <td>3.822564</td>\n",
       "      <td>2.750004</td>\n",
       "      <td>4.210648</td>\n",
       "      <td>3.195352</td>\n",
       "      <td>3.263384</td>\n",
       "      <td>3.226213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.028276</td>\n",
       "      <td>4.151319</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.583655</td>\n",
       "      <td>2.820859</td>\n",
       "      <td>2.327025</td>\n",
       "      <td>2.896990</td>\n",
       "      <td>3.921444</td>\n",
       "      <td>3.261746</td>\n",
       "      <td>1.292208</td>\n",
       "      <td>6.060862</td>\n",
       "      <td>3.102732</td>\n",
       "      <td>4.927852</td>\n",
       "      <td>3.230224</td>\n",
       "      <td>1.068353</td>\n",
       "      <td>2.544864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586667</th>\n",
       "      <td>4.507507</td>\n",
       "      <td>4.525337</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.797992</td>\n",
       "      <td>4.408144</td>\n",
       "      <td>3.942313</td>\n",
       "      <td>3.207572</td>\n",
       "      <td>3.014376</td>\n",
       "      <td>2.259826</td>\n",
       "      <td>3.485083</td>\n",
       "      <td>7.321132</td>\n",
       "      <td>4.829626</td>\n",
       "      <td>4.268849</td>\n",
       "      <td>4.802366</td>\n",
       "      <td>3.543452</td>\n",
       "      <td>2.489765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586668</th>\n",
       "      <td>4.967984</td>\n",
       "      <td>3.997606</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.682946</td>\n",
       "      <td>5.384268</td>\n",
       "      <td>4.684574</td>\n",
       "      <td>2.361519</td>\n",
       "      <td>3.863641</td>\n",
       "      <td>2.298209</td>\n",
       "      <td>4.028028</td>\n",
       "      <td>7.587534</td>\n",
       "      <td>5.696170</td>\n",
       "      <td>5.347732</td>\n",
       "      <td>3.508698</td>\n",
       "      <td>5.025582</td>\n",
       "      <td>3.510531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586669</th>\n",
       "      <td>2.513383</td>\n",
       "      <td>4.620409</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3.026764</td>\n",
       "      <td>4.275283</td>\n",
       "      <td>6.560318</td>\n",
       "      <td>5.074978</td>\n",
       "      <td>4.096501</td>\n",
       "      <td>2.595194</td>\n",
       "      <td>1.959881</td>\n",
       "      <td>3.780759</td>\n",
       "      <td>4.040783</td>\n",
       "      <td>5.324980</td>\n",
       "      <td>2.569675</td>\n",
       "      <td>2.156150</td>\n",
       "      <td>2.363571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586670</th>\n",
       "      <td>4.446574</td>\n",
       "      <td>4.300193</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>4.233162</td>\n",
       "      <td>4.264293</td>\n",
       "      <td>5.235272</td>\n",
       "      <td>1.241963</td>\n",
       "      <td>4.270548</td>\n",
       "      <td>4.267136</td>\n",
       "      <td>2.311171</td>\n",
       "      <td>6.389156</td>\n",
       "      <td>5.381336</td>\n",
       "      <td>5.270690</td>\n",
       "      <td>4.331300</td>\n",
       "      <td>3.387321</td>\n",
       "      <td>5.655649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586671</th>\n",
       "      <td>3.847120</td>\n",
       "      <td>3.737806</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.963249</td>\n",
       "      <td>5.151583</td>\n",
       "      <td>2.118279</td>\n",
       "      <td>3.689731</td>\n",
       "      <td>4.534883</td>\n",
       "      <td>3.179928</td>\n",
       "      <td>3.346431</td>\n",
       "      <td>3.523212</td>\n",
       "      <td>3.196295</td>\n",
       "      <td>6.148501</td>\n",
       "      <td>0.977880</td>\n",
       "      <td>3.550117</td>\n",
       "      <td>3.168971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>586344 rows × 16 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=11, algorithm='ball_tree')\n",
    "nn.fit(encoded_songs)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', n_neighbors=11)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "len(encoded_songs[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Saving the encoded_songs embedding as a df and the Nearest Neighbours model for use in our app\n",
    "import pickle\n",
    "\n",
    "embeddings_df = pd.DataFrame(encoded_songs, index=df.index)\n",
    "\n",
    "# Saving the embeddings to the disk\n",
    "embeddings_df.to_csv('./saved_models/embeddings_df_001.csv')\n",
    "\n",
    "# save the model to disk\n",
    "filename = './saved_models/nearestneigh_001.sav'\n",
    "pickle.dump(nn, open(filename, 'wb'))\n",
    "\n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "embeddings_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              0         1    2         3         4         5         6   \\\n",
       "0       3.470329  1.184762 -0.0  3.144354  1.041142  2.506661  2.963388   \n",
       "1       2.950846  0.692970 -0.0  1.260800  2.372062  1.424065  1.766102   \n",
       "2       4.472703  6.916587 -0.0  3.051311  4.255427  4.078246  6.610500   \n",
       "3       0.533942  1.481426 -0.0  4.247745  1.795213  3.724570  6.466702   \n",
       "4       2.028276  4.151319 -0.0  2.583655  2.820859  2.327025  2.896990   \n",
       "...          ...       ...  ...       ...       ...       ...       ...   \n",
       "586667  4.507507  4.525337 -0.0  1.797992  4.408144  3.942313  3.207572   \n",
       "586668  4.967984  3.997606 -0.0  2.682946  5.384268  4.684574  2.361519   \n",
       "586669  2.513383  4.620409 -0.0  3.026764  4.275283  6.560318  5.074978   \n",
       "586670  4.446574  4.300193 -0.0  4.233162  4.264293  5.235272  1.241963   \n",
       "586671  3.847120  3.737806 -0.0  2.963249  5.151583  2.118279  3.689731   \n",
       "\n",
       "              7         8         9         10        11        12        13  \\\n",
       "0       1.308488  1.876889  1.609339  5.813118  4.160755  2.801373  6.369508   \n",
       "1       3.973808  1.751205  1.503640  4.932892  4.102685  1.840484  4.968472   \n",
       "2       5.424572  5.226394  2.885791  4.807835  8.209723  6.890685  2.668237   \n",
       "3       2.883682  2.688429  2.568929  3.822564  2.750004  4.210648  3.195352   \n",
       "4       3.921444  3.261746  1.292208  6.060862  3.102732  4.927852  3.230224   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "586667  3.014376  2.259826  3.485083  7.321132  4.829626  4.268849  4.802366   \n",
       "586668  3.863641  2.298209  4.028028  7.587534  5.696170  5.347732  3.508698   \n",
       "586669  4.096501  2.595194  1.959881  3.780759  4.040783  5.324980  2.569675   \n",
       "586670  4.270548  4.267136  2.311171  6.389156  5.381336  5.270690  4.331300   \n",
       "586671  4.534883  3.179928  3.346431  3.523212  3.196295  6.148501  0.977880   \n",
       "\n",
       "              14        15  \n",
       "0       4.479137  2.048349  \n",
       "1       4.601698  2.108422  \n",
       "2       4.231436  3.645994  \n",
       "3       3.263384  3.226213  \n",
       "4       1.068353  2.544864  \n",
       "...          ...       ...  \n",
       "586667  3.543452  2.489765  \n",
       "586668  5.025582  3.510531  \n",
       "586669  2.156150  2.363571  \n",
       "586670  3.387321  5.655649  \n",
       "586671  3.550117  3.168971  \n",
       "\n",
       "[586344 rows x 16 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.470329</td>\n",
       "      <td>1.184762</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3.144354</td>\n",
       "      <td>1.041142</td>\n",
       "      <td>2.506661</td>\n",
       "      <td>2.963388</td>\n",
       "      <td>1.308488</td>\n",
       "      <td>1.876889</td>\n",
       "      <td>1.609339</td>\n",
       "      <td>5.813118</td>\n",
       "      <td>4.160755</td>\n",
       "      <td>2.801373</td>\n",
       "      <td>6.369508</td>\n",
       "      <td>4.479137</td>\n",
       "      <td>2.048349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.950846</td>\n",
       "      <td>0.692970</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.260800</td>\n",
       "      <td>2.372062</td>\n",
       "      <td>1.424065</td>\n",
       "      <td>1.766102</td>\n",
       "      <td>3.973808</td>\n",
       "      <td>1.751205</td>\n",
       "      <td>1.503640</td>\n",
       "      <td>4.932892</td>\n",
       "      <td>4.102685</td>\n",
       "      <td>1.840484</td>\n",
       "      <td>4.968472</td>\n",
       "      <td>4.601698</td>\n",
       "      <td>2.108422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.472703</td>\n",
       "      <td>6.916587</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3.051311</td>\n",
       "      <td>4.255427</td>\n",
       "      <td>4.078246</td>\n",
       "      <td>6.610500</td>\n",
       "      <td>5.424572</td>\n",
       "      <td>5.226394</td>\n",
       "      <td>2.885791</td>\n",
       "      <td>4.807835</td>\n",
       "      <td>8.209723</td>\n",
       "      <td>6.890685</td>\n",
       "      <td>2.668237</td>\n",
       "      <td>4.231436</td>\n",
       "      <td>3.645994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.533942</td>\n",
       "      <td>1.481426</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>4.247745</td>\n",
       "      <td>1.795213</td>\n",
       "      <td>3.724570</td>\n",
       "      <td>6.466702</td>\n",
       "      <td>2.883682</td>\n",
       "      <td>2.688429</td>\n",
       "      <td>2.568929</td>\n",
       "      <td>3.822564</td>\n",
       "      <td>2.750004</td>\n",
       "      <td>4.210648</td>\n",
       "      <td>3.195352</td>\n",
       "      <td>3.263384</td>\n",
       "      <td>3.226213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.028276</td>\n",
       "      <td>4.151319</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.583655</td>\n",
       "      <td>2.820859</td>\n",
       "      <td>2.327025</td>\n",
       "      <td>2.896990</td>\n",
       "      <td>3.921444</td>\n",
       "      <td>3.261746</td>\n",
       "      <td>1.292208</td>\n",
       "      <td>6.060862</td>\n",
       "      <td>3.102732</td>\n",
       "      <td>4.927852</td>\n",
       "      <td>3.230224</td>\n",
       "      <td>1.068353</td>\n",
       "      <td>2.544864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586667</th>\n",
       "      <td>4.507507</td>\n",
       "      <td>4.525337</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.797992</td>\n",
       "      <td>4.408144</td>\n",
       "      <td>3.942313</td>\n",
       "      <td>3.207572</td>\n",
       "      <td>3.014376</td>\n",
       "      <td>2.259826</td>\n",
       "      <td>3.485083</td>\n",
       "      <td>7.321132</td>\n",
       "      <td>4.829626</td>\n",
       "      <td>4.268849</td>\n",
       "      <td>4.802366</td>\n",
       "      <td>3.543452</td>\n",
       "      <td>2.489765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586668</th>\n",
       "      <td>4.967984</td>\n",
       "      <td>3.997606</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.682946</td>\n",
       "      <td>5.384268</td>\n",
       "      <td>4.684574</td>\n",
       "      <td>2.361519</td>\n",
       "      <td>3.863641</td>\n",
       "      <td>2.298209</td>\n",
       "      <td>4.028028</td>\n",
       "      <td>7.587534</td>\n",
       "      <td>5.696170</td>\n",
       "      <td>5.347732</td>\n",
       "      <td>3.508698</td>\n",
       "      <td>5.025582</td>\n",
       "      <td>3.510531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586669</th>\n",
       "      <td>2.513383</td>\n",
       "      <td>4.620409</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3.026764</td>\n",
       "      <td>4.275283</td>\n",
       "      <td>6.560318</td>\n",
       "      <td>5.074978</td>\n",
       "      <td>4.096501</td>\n",
       "      <td>2.595194</td>\n",
       "      <td>1.959881</td>\n",
       "      <td>3.780759</td>\n",
       "      <td>4.040783</td>\n",
       "      <td>5.324980</td>\n",
       "      <td>2.569675</td>\n",
       "      <td>2.156150</td>\n",
       "      <td>2.363571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586670</th>\n",
       "      <td>4.446574</td>\n",
       "      <td>4.300193</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>4.233162</td>\n",
       "      <td>4.264293</td>\n",
       "      <td>5.235272</td>\n",
       "      <td>1.241963</td>\n",
       "      <td>4.270548</td>\n",
       "      <td>4.267136</td>\n",
       "      <td>2.311171</td>\n",
       "      <td>6.389156</td>\n",
       "      <td>5.381336</td>\n",
       "      <td>5.270690</td>\n",
       "      <td>4.331300</td>\n",
       "      <td>3.387321</td>\n",
       "      <td>5.655649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586671</th>\n",
       "      <td>3.847120</td>\n",
       "      <td>3.737806</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.963249</td>\n",
       "      <td>5.151583</td>\n",
       "      <td>2.118279</td>\n",
       "      <td>3.689731</td>\n",
       "      <td>4.534883</td>\n",
       "      <td>3.179928</td>\n",
       "      <td>3.346431</td>\n",
       "      <td>3.523212</td>\n",
       "      <td>3.196295</td>\n",
       "      <td>6.148501</td>\n",
       "      <td>0.977880</td>\n",
       "      <td>3.550117</td>\n",
       "      <td>3.168971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>586344 rows × 16 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# testing with Beautiful Day - U2\n",
    "\n",
    "raw_df = pd.read_csv('./Data/tracks.csv')\n",
    "test_songs = {'Beautiful Day': '0gzqZ9d1jIKo9psEIthwXe'}\n",
    "for name, id in test_songs.items():\n",
    "    query = raw_df.index[raw_df['id'] == '0gzqZ9d1jIKo9psEIthwXe'][0]\n",
    "    test_song = pd.DataFrame([raw_df.iloc[query, :]])\n",
    "\n",
    "    score, close_image_idx = nn.kneighbors(encoded_songs[query].reshape(1,-1))\n",
    "    k_closest_songs_indexes = close_image_idx[0][1:]\n",
    "\n",
    "    print('Top 10 recommendations for {} by {}:\\n'.format(name, test_song.artists))\n",
    "    for i in range(len(k_closest_songs_indexes)):\n",
    "        song = raw_df.iloc[k_closest_songs_indexes[i], :]\n",
    "        print('{} by {} with a score of {}\\n'.format(song['name'], song.artists, score[0][1:][i]))\n",
    "        print('URL: https://open.spotify.com/track/{}\\n'.format(song['id']))\n",
    "    print('___________________________________________________________________________________\\n')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Testing our model with multiple songs\n",
    "raw_df = pd.read_csv('./Data/tracks.csv')\n",
    "\n",
    "test_songs = {'California Love': '0LyLz6XsVs6wz85dK0S6EG', 'The Gambler': '5KqldkCunQ2rWxruMEtGh0', \n",
    "              'Eye of the Tiger': '2HHtWyy5CgaQbC7XSoOb0e', 'You are my Sunshine': '4ApTr4wbyOucy2lbAvyuuV'}\n",
    "\n",
    "# Loop over all our test songs and get the recommendations\n",
    "recommendations = {}\n",
    "\n",
    "print('DeepSpot\\'s recommendation system:\\n\\n')\n",
    "\n",
    "for name, id in test_songs.items():\n",
    "    query = raw_df.index[raw_df['id'] == id][0]\n",
    "    test_song = pd.DataFrame([raw_df.iloc[query, :]])\n",
    "    \n",
    "    score, close_image_idx = nn.kneighbors(encoded_songs[query].reshape(1,-1))\n",
    "    k_closest_songs_indexes = close_image_idx[0][1:]\n",
    "    \n",
    "    print('Top 10 recommendations for {} by {}:\\n'.format(name, test_song.artists))\n",
    "    for i in range(len(k_closest_songs_indexes)):\n",
    "        song = raw_df.iloc[k_closest_songs_indexes[i], :]\n",
    "        print('{} by {} with a score of {}\\n'.format(song['name'], song.artists, score[0][i]))\n",
    "        print('URL: https://open.spotify.com/track/{}\\n'.format(song['id']))\n",
    "    print('___________________________________________________________________________________\\n')\n",
    "    \n",
    "    recommendations[name] = k_closest_songs_indexes\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Deep variational encoder\n",
    "\n",
    "encoded_dim = 16\n",
    "\n",
    "input_song = Input(shape = (df.shape[1], ))\n",
    "\n",
    "encoded_h1 = Dense(64, activation = 'relu')(input_song)\n",
    "encoded_h2 = Dense(32, activation = 'relu')(encoded_h1)\n",
    "encoded = Dense(encoded_dim, activation = 'relu')(encoded_h2)\n",
    "\n",
    "decoded_h1 = Dense(32, activation='relu')(encoded)\n",
    "decoded_h2 = Dense(64, activation='relu')(decoded_h1)\n",
    "decoded = Dense(df.shape[1], activation='sigmoid')(decoded_h2)\n",
    "\n",
    "deep_autoencoder = Model(input_song, decoded) # techincally autoencoder_and_decoder\n",
    "\n",
    "deep_encoder = Model(input_song, encoded) # Half of the autoencoder\n",
    "\n",
    "deep_autoencoder.compile(optimizer='nadam', loss='mean_squared_error')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3)\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "logdir = os.path.join(\"logs\", f\"SimpleAutoencoder-{now}\")\n",
    "tensorboard = TensorBoard(log_dir=logdir)\n",
    "\n",
    "deep_autoencoder.fit(df, # input image to encoder\n",
    "                df, # provide input image to decoder so the model learns how to reconstruct the input image \n",
    "                epochs=100,\n",
    "                validation_split=.2,\n",
    "                callbacks=[stop, tensorboard])"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deep NN Summary\n",
    "Val loss = 0.2960"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "decoded_songs = autoencoder.predict(df)\n",
    "encoded_songs = encoder.predict(df)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=11, algorithm='ball_tree')\n",
    "nn.fit(encoded_songs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encoded_songs.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # Testing\n",
    "# raw_df = pd.read_csv('./Data/tracks.csv')\n",
    "\n",
    "# test_songs = {'California Love': '0LyLz6XsVs6wz85dK0S6EG', 'The Gambler': '5KqldkCunQ2rWxruMEtGh0', \n",
    "#               'Eye of the Tiger': '2HHtWyy5CgaQbC7XSoOb0e', 'You are my Sunshine': '4ApTr4wbyOucy2lbAvyuuV'}\n",
    "\n",
    "# # Loop over all our test songs and get the recommendations\n",
    "# recommendations = {}\n",
    "\n",
    "# print('DeepSpot\\'s recommendation system:\\n\\n')\n",
    "\n",
    "# for name, id in test_songs.items():\n",
    "#     query = raw_df.index[raw_df['id'] == id][0]\n",
    "#     test_song = pd.DataFrame([raw_df.iloc[query, :]])\n",
    "    \n",
    "#     score, close_image_idx = nn.kneighbors(encoded_songs[query].reshape(1,-1))\n",
    "#     k_closest_songs_indexes = close_image_idx[0][1:]\n",
    "    \n",
    "#     print('Top 10 recommendations for {} by {}:\\n'.format(name, test_song.artists))\n",
    "#     for i in range(len(k_closest_songs_indexes)):\n",
    "#         song = raw_df.iloc[k_closest_songs_indexes[i], :]\n",
    "#         print('{} by {} with a score of {}\\n'.format(song['name'], song.artists, score[0][i]))\n",
    "#         print('URL: https://open.spotify.com/track/{}\\n'.format(song['id']))\n",
    "#     print('___________________________________________________________________________________\\n')\n",
    "    \n",
    "#     recommendations[name] = k_closest_songs_indexes\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training a Variational Autoencoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Variational Autoencoder\n",
    "from keras import layers\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"\n",
    "    Uses (z_mean, z_log_var) to sample a z vector from the latent N-dimensional Normal distribution\n",
    "    i.e. Z is the vector encoding a digit.\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # recall from the video that z_mean and z_log_var are vectors \n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        \n",
    "        # sample from an N-dimensional normal distribution\n",
    "        # epsilon is given shape (batch, dim) because we are adding it to z_mean which has shape (batch, dim)\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        \n",
    "        # output of tf.exp() is a vector \n",
    "        simga = tf.exp(0.5 * z_log_var)\n",
    "        \n",
    "        # this is our hideen latent vector made up of a mean and variance vector \n",
    "        # variance vector is scaled by epsilon, which is sampled from a normal distribtuion \n",
    "        # the the video guy said \"stocastic\" in reference to epsilon, he meant random \n",
    "        Z = z_mean + simga * epsilon\n",
    "        \n",
    "        # return hidden latent vector \n",
    "        return Z"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# recall from the video that the more dimensions that our latent vector has\n",
    "# the better the results of our model \n",
    "latent_dim = 16\n",
    "\n",
    "# shape of our input data\n",
    "# we are creating our input layer using Keras's Input() class\n",
    "# the only thing that input layers really do is define the dimensionality of the input data for the model\n",
    "encoder_inputs = Input(shape=(df.shape[1], ))\n",
    "\n",
    "# these are the hidden layers\n",
    "\n",
    "# pass data vector into FCFF layer \n",
    "encoded = Dense(encoded_dim, activation = 'relu')(encoder_inputs)\n",
    "\n",
    "# recall that ordinarly the output of the last encoding layer is the latent vector \n",
    "# but here we are creating two output layers for our encoder - one for the mean and one for the log variance \n",
    "# returns a 2-dim mean vector\n",
    "z_mean = Dense(latent_dim, name=\"z_mean\")(encoded)\n",
    "# returns a 2-dim log variance vecotr \n",
    "z_log_var = Dense(latent_dim, name=\"z_log_var\")(encoded)\n",
    "# pass mean and variance vector into Sampling class in order to create the Z vector,  Z = mean + var * epsilon\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "# ok, now let's put it all together \n",
    "# this is our encoder model \n",
    "# inputs are the original images\n",
    "# outputs are the Z vectors: mean, log variance, and the complete Z, i.e. vector Z = mean + var * epsilon\n",
    "encoder = Model(inputs=encoder_inputs, outputs=[z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# the input layer to our decoder has the same dimensionality as the latent vector\n",
    "# because the latent vector is the input to the decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,))\n",
    "\n",
    "# these are the hidden layers of our decoder \n",
    "# the data at this point is in a vector, the Z latent vector \n",
    "\n",
    "# this is the final layer in out decoder\n",
    "# therefore this layer outputs the reconstruction of the original image \n",
    "decoder_outputs = Dense(df.shape[1], activation='sigmoid')(latent_inputs)\n",
    "\n",
    "# this is our decoder model \n",
    "decoder = Model(inputs=latent_inputs, outputs=decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        \"\"\"\n",
    "        This class build a Variational Auto-Encoder. It accepts an Encoder and Decoder model as input. \n",
    "        \n",
    "        Note\n",
    "        ----\n",
    "        This VAE class is inheriting Keras's Model API so that it can use the Model class methods \n",
    "\n",
    "        \"\"\"\n",
    "        # how python 3 handels inheritance \n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        # set encoder model as class attribute\n",
    "        self.encoder = encoder\n",
    "        # set decoder model as class attribute \n",
    "        self.decoder = decoder\n",
    "        # set mean function as class attribute - this calculates the total loss\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        # set mean function as class attribute - this calculates the reconstruction loss\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        # set mean function as class attribute - this calculates the kl loss\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"\n",
    "        Returns all loses in a list\n",
    "        \"\"\"\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Training our model via gradient descent and back-propagation \n",
    "        \"\"\"\n",
    "        \n",
    "        # we used tf.GradientTape() in Sprint 2 Module 2 to run Gradient Descent from scratch \n",
    "        with tf.GradientTape() as tape:\n",
    "            # pass input data into encoder model \n",
    "            # output of encoder model is the hidden state distribution parameters and hidden state vector \n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            \n",
    "            # pass hidden state vector into decoder model \n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            # calculate the reconstruction loss \n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.MeanSquaredError(data, reconstruction)#, axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # calculate the kl loss\n",
    "            #                (1 + z_simga   - (z_mean)^2        - e^(z_simga) ) \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            # recall that we used tf.reduce_sum() in Sprint 2 Module 4 assignment \n",
    "            # it takes the sum of the vector components \n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            \n",
    "            # calculate the total loss by adding the two loss components \n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            \n",
    "        # now that we have calculated the loss function, we can perform Gradient Descent\n",
    "        # we are passing in the loss function and the weights that we want to update via Gradeint Descent \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        # log the total loss\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        \n",
    "        # log the reconsgrution loss \n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        \n",
    "        # log the kl loss \n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        # return all the losses in a dictionary \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# instantiate a Variational Auto-Encoder model \n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "# complie the model \n",
    "vae.compile(optimizer=keras.optimizers.Nadam())\n",
    "\n",
    "# train the model weights \n",
    "vae.fit(df, epochs=10, workers=10)\n",
    "# if you have access to multiple processors, set parameter `workers` to N - 1\n",
    "# where N is the total number of processors that you have "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p saved_model\n",
    "autoencoder.save('saved_model/autoencoder_002')\n",
    "encoder.save('saved_model/encoder_002')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('U4-S2-NN': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "interpreter": {
   "hash": "02e1903c3f39b96bcefa6d49f9b3f85f880bb59f74a50c875b853cacbce3839e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}